{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-28 15:03:50--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 228145 (223K) [text/plain]\n",
      "Saving to: ‘names.txt’\n",
      "\n",
      "names.txt           100%[===================>] 222.80K  --.-KB/s    in 0.007s  \n",
      "\n",
      "2023-04-28 15:03:50 (30.4 MB/s) - ‘names.txt’ saved [228145/228145]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(''.join(words))))\n",
    "stoi = {ch:i+1 for i,ch in enumerate(vocab)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:ch for ch,i in stoi.items()}\n",
    "encode = lambda x: [stoi[i] for i in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182778, 3]) torch.Size([182778])\n",
      "torch.Size([22633, 3]) torch.Size([22633])\n",
      "torch.Size([22735, 3]) torch.Size([22735])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "def build_data(words):\n",
    "    X, Y = [], []\n",
    "    for word in words:\n",
    "        context = [0] * block_size\n",
    "        for char in word + '.':\n",
    "            ix = stoi[char]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "n1 = int(len(words) * 0.8)\n",
    "n2 = int(len(words) * 0.9)\n",
    "Xtr, Ytr = build_data(words[:n1])\n",
    "Xte, Yte =  build_data(words[n1:n2])\n",
    "Xdev , Ydev = build_data(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, IN, OUT, bias=True):\n",
    "        self.w = torch.randn(IN, OUT) / IN ** 0.5 \n",
    "        self.b = torch.zeros(OUT) if bias else None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.w \n",
    "        if self.b is not None:\n",
    "            self.out += self.b\n",
    "        return self.out\n",
    "            \n",
    "    def parameters(self):\n",
    "        return [self.w] + ([] if self.b is None else [self.b])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(0,keepdim=True)\n",
    "            xvar = x.var(0,keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_mean\n",
    "        xnorm = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xnorm + self.beta\n",
    "        if self.training:\n",
    "            self.running_mean = self.momentum * self.running_mean +  self.momentum * xmean\n",
    "            self.running_var = self.momentum * self.running_var +  self.momentum * xvar\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "            \n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 168870\n"
     ]
    }
   ],
   "source": [
    "n_emb = 10\n",
    "n_hidden = 200\n",
    "vocab_size = len(stoi)\n",
    "g = torch.Generator().manual_seed(42)\n",
    "C = torch.randn(vocab_size, n_emb, generator=g)\n",
    "Layers = [Linear(n_emb * block_size, n_hidden), BatchNorm1d(n_hidden), Tanh(), \n",
    "          Linear(n_hidden, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n",
    "          Linear(n_hidden, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n",
    "          Linear(n_hidden, n_hidden), BatchNorm1d(n_hidden),Tanh(),\n",
    "          Linear(n_hidden, n_hidden)]\n",
    "\n",
    "parameters = [C] + [p for layer in Layers for p in layer.parameters()]\n",
    "print(f\"Total parameters:\", sum(p.numel() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 0/20000   : 5.5347442626953125\n",
      ". 1000/20000   : 4.730920791625977\n",
      ". 2000/20000   : 3.9596426486968994\n",
      ". 3000/20000   : 3.2961580753326416\n",
      ". 4000/20000   : 3.987142324447632\n",
      ". 5000/20000   : 3.265638828277588\n",
      ". 6000/20000   : 3.225757122039795\n",
      ". 7000/20000   : 3.1466031074523926\n",
      ". 8000/20000   : 3.5089707374572754\n",
      ". 9000/20000   : 2.958566188812256\n",
      ". 10000/20000   : 3.0267727375030518\n",
      ". 11000/20000   : 2.728703498840332\n",
      ". 12000/20000   : 2.8094024658203125\n",
      ". 13000/20000   : 3.125795602798462\n",
      ". 14000/20000   : 2.973686933517456\n",
      ". 15000/20000   : 3.3151891231536865\n",
      ". 16000/20000   : 2.3588993549346924\n",
      ". 17000/20000   : 2.698923110961914\n",
      ". 18000/20000   : 2.6301627159118652\n",
      ". 19000/20000   : 2.8570263385772705\n"
     ]
    }
   ],
   "source": [
    "steps = 20000\n",
    "batch_size = 25\n",
    "lr = 0.001\n",
    "for i in range(steps):\n",
    "    #get minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb , Yb = Xtr[ix], Ytr[ix]\n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "    for layer in Layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb)\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.data += - lr * p.grad\n",
    "    if i%1000==0:\n",
    "        print(f\". {i}/{steps}   : {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  for layer in Layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(5)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
    "      x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "      \n",
    "\n",
    "      hpreact = x @ W1 \n",
    "      hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "    \n",
    "  \n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "      ix = torch.multinomial(probs, num_samples=1).item()\n",
    "      # shift the context window and track the samples\n",
    "      context = context[1:] + [ix]\n",
    "      # if we sample the special '.' token, break\n",
    "      if ix == 0:\n",
    "        break\n",
    "      out.append(ix)\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
