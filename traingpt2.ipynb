{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlhf8a8QIk/3duQjW4Lc+e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hardeybisey/neural-network/blob/main/traingpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZXGlz9IoIEZ",
        "outputId": "b6ac8e30-464c-4f3b-bdd6-dc6927e1214b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.13.3\n"
          ]
        }
      ],
      "source": [
        "# !pip install tokenizers\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tokenizers import Tokenizer,PreTokenizedString,NormalizedString, pre_tokenizers, CharBPETokenizer , decoders,  normalizers,ByteLevelBPETokenizer\n",
        "from tokenizers.models import BPE , WordPiece\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace, PreTokenizer\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
        "torch.manual_seed(45562)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8lih-fmoQik",
        "outputId": "0951cfb1-48b8-466b-e4a7-9692056f572d"
      },
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6fa5ac3590>"
            ]
          },
          "metadata": {},
          "execution_count": 314
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the dataset"
      ],
      "metadata": {
        "id": "VUqBry_gJGRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('input.txt', 'r', encoding='utf-8').read()\n",
        "print(\"length of dataset in characters: \", len(text))\n",
        "# print(f\"\\n{text[:200]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSRy14hJJFoi",
        "outputId": "dd033bb3-7e77-4fcd-d0a4-fac99411e379"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a custom tokenizer with hugging face  tokenizer API"
      ],
      "metadata": {
        "id": "DnGgF5W0I4x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.Split(\"\\n\",'merged_with_previous'),Whitespace()])"
      ],
      "metadata": {
        "id": "T-oBF0Y_aJWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_tokenizer.pre_tokenize_str(text[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujmb0dxhasjR",
        "outputId": "b5573b03-132d-434f-aefe-d3c976b49bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('First Citizen:\\n', (0, 15)),\n",
              " ('Before we proceed any further, hear', (15, 50))]"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomPreTokenizer:\n",
        "    \"A pre_tokenizer that preserves the preserves the new line character\"\n",
        "    def line_split(self, i:int, normalized_string):\n",
        "        splits = []\n",
        "        for idx, token in enumerate(normalized_string.split(\" \")):\n",
        "          if \"\\n\" in token:\n",
        "              idy = token.index(\"\\n\")\n",
        "              subtokens = token.split(\"\\n\")\n",
        "              print(token)\n",
        "              for i, subtoken in enumerate(subtokens):\n",
        "                  if i > 0:\n",
        "                      splits.append(normalized_string[idx][idy])\n",
        "                  if subtoken:\n",
        "                      splits.append(normalized_string[idx][i])\n",
        "          else:\n",
        "              splits.append(normalized_string[idx])\n",
        "        return splits\n",
        "\n",
        "    def pre_tokenize(self, pretok):\n",
        "        pretok.split(self.line_split)"
      ],
      "metadata": {
        "id": "mhJ7SvNScEAS"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c = CustomPreTokenizer()\n",
        "# c.line_split(0,text[:50])\n",
        "normalized_string = text[:100]\n",
        "splits = []\n",
        "current = 0\n",
        "for token in normalized_string.split(\" \"):\n",
        "  if \"\\n\" in token:\n",
        "      idy = token.index(\"\\n\")\n",
        "      subtokens = token.split(\"\\n\")\n",
        "      print('norma index', normalized_string.find(token))\n",
        "      print('current index', current)\n",
        "      print('token', token)\n",
        "      for i, subtoken in enumerate(subtokens):\n",
        "          if i%2 != 0:\n",
        "              print('new line token',subtoken)\n",
        "              splits.append(normalized_string[current:current+2])\n",
        "              current += 2\n",
        "              splits.append(normalized_string[current:current+len(subtoken)])\n",
        "              current += len(subtoken)\n",
        "          else:\n",
        "              print(subtoken)\n",
        "              splits.append(normalized_string[current:current+len(subtoken)])\n",
        "              current += len(subtoken)\n",
        "  else:\n",
        "      splits.append(normalized_string[current:current+len(token)])\n",
        "      current += len(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B899jZBHBUr",
        "outputId": "d02c561b-fd02-4ca5-fff7-e086d6e69e47"
      },
      "execution_count": 409,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "norma index 6\n",
            "current index 5\n",
            "token Citizen:\n",
            "Before\n",
            "Citizen:\n",
            "new line token Before\n",
            "norma index 54\n",
            "current index 47\n",
            "token speak.\n",
            "\n",
            "All:\n",
            "Speak,\n",
            "speak.\n",
            "new line token \n",
            "All:\n",
            "new line token Speak,\n",
            "norma index 74\n",
            "current index 67\n",
            "token speak.\n",
            "\n",
            "First\n",
            "speak.\n",
            "new line token \n",
            "First\n",
            "norma index 88\n",
            "current index 80\n",
            "token Citizen:\n",
            "You\n",
            "Citizen:\n",
            "new line token You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'myname is a boy'.find('boy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOPa1Ux3PNrQ",
        "outputId": "89025820-a589-4b0d-86a1-0245c975206a"
      },
      "execution_count": 410,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 410
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrpqvdX7JPqm",
        "outputId": "6f6654aa-a3bd-4828-8014-326e34f5b453"
      },
      "execution_count": 406,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['First',\n",
              " ' Citizen',\n",
              " ':\\n',\n",
              " 'Before',\n",
              " ' w',\n",
              " 'e proce',\n",
              " 'ed ',\n",
              " 'any furt',\n",
              " 'her,',\n",
              " ' h',\n",
              " 'ear me',\n",
              " ' s',\n",
              " '',\n",
              " 'peak',\n",
              " '.\\n',\n",
              " '\\nAll:\\n',\n",
              " 'Speak,',\n",
              " ' s',\n",
              " '',\n",
              " 'peak.',\n",
              " '\\n\\nFirst ',\n",
              " 'Ci',\n",
              " 'tiz']"
            ]
          },
          "metadata": {},
          "execution_count": 406
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'speak.\\n\\nAll:\\nSpeak,'.split(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em87HzWKNHtG",
        "outputId": "c2f07c30-1efc-4147-805d-a11bb7c1ce3f"
      },
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['speak.', '', 'All:', 'Speak,']"
            ]
          },
          "metadata": {},
          "execution_count": 386
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = text[:500].split(\" \")\n",
        "a#[1].split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRXjk-AAKG47",
        "outputId": "a7fb95e9-26a1-4a6c-f38a-e5005f145088"
      },
      "execution_count": 383,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['First',\n",
              " 'Citizen:\\nBefore',\n",
              " 'we',\n",
              " 'proceed',\n",
              " 'any',\n",
              " 'further,',\n",
              " 'hear',\n",
              " 'me',\n",
              " 'speak.\\n\\nAll:\\nSpeak,',\n",
              " 'speak.\\n\\nFirst',\n",
              " 'Citizen:\\nYou',\n",
              " 'are',\n",
              " 'all',\n",
              " 'resolved',\n",
              " 'rather',\n",
              " 'to',\n",
              " 'die',\n",
              " 'than',\n",
              " 'to',\n",
              " 'famish?\\n\\nAll:\\nResolved.',\n",
              " 'resolved.\\n\\nFirst',\n",
              " 'Citizen:\\nFirst,',\n",
              " 'you',\n",
              " 'know',\n",
              " 'Caius',\n",
              " 'Marcius',\n",
              " 'is',\n",
              " 'chief',\n",
              " 'enemy',\n",
              " 'to',\n",
              " 'the',\n",
              " 'people.\\n\\nAll:\\nWe',\n",
              " \"know't,\",\n",
              " 'we',\n",
              " \"know't.\\n\\nFirst\",\n",
              " 'Citizen:\\nLet',\n",
              " 'us',\n",
              " 'kill',\n",
              " 'him,',\n",
              " 'and',\n",
              " \"we'll\",\n",
              " 'have',\n",
              " 'corn',\n",
              " 'at',\n",
              " 'our',\n",
              " 'own',\n",
              " \"price.\\nIs't\",\n",
              " 'a',\n",
              " 'verdict?\\n\\nAll:\\nNo',\n",
              " 'more',\n",
              " 'talking',\n",
              " \"on't;\",\n",
              " 'let',\n",
              " 'it',\n",
              " 'be',\n",
              " 'done:',\n",
              " 'away,',\n",
              " 'away!\\n\\nSecond',\n",
              " 'Citizen:\\nOne',\n",
              " 'word,',\n",
              " 'good',\n",
              " 'citizens.\\n\\nFirst',\n",
              " 'Citizen:\\nWe',\n",
              " 'are',\n",
              " 'accounted',\n",
              " 'poor']"
            ]
          },
          "metadata": {},
          "execution_count": 383
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w[][]"
      ],
      "metadata": {
        "id": "UqV8Qw0zLpRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUBZFV4_JfKo",
        "outputId": "bbf47b1e-50cf-4bc5-aadd-977814802335"
      },
      "execution_count": 353,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Citizen:', 'Before']"
            ]
          },
          "metadata": {},
          "execution_count": 353
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:100].split(\" \")[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HgbaPfgjJKPf",
        "outputId": "b16a92d9-258c-44e7-fdad-73e37ecdab9e"
      },
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Citizen:\\nBefore'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 345
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = text[:100]"
      ],
      "metadata": {
        "id": "5sjR1m1nI74z"
      },
      "execution_count": 361,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Jw6WajdiI3G-",
        "outputId": "e06a0097-9cb6-4e4e-ce80-2fde650b62fb"
      },
      "execution_count": 342,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 342
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "3VeyTUms67F8",
        "outputId": "ede725b4-6be3-4858-872d-189468171a41"
      },
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 328
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok = ByteLevelBPETokenizer()\n",
        "# tok.normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
        "tok.pre_tokenizer = PreTokenizer.custom(CustomPreTokenizer())\n",
        "# tok.decoder = Decoder.custom(CustomDecoder())\n",
        "print(tok.pre_tokenizer.pre_tokenize_str(text[:50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "gjbsuF4a366z",
        "outputId": "799de8ee-c38f-4c4a-87da-a9196d68ba44"
      },
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-319-8f4aea457ef6>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustomPreTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# tok.decoder = Decoder.custom(CustomDecoder())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenize_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: Exception: TypeError: NormalizedString.split() missing 1 required positional argument: 'behavior'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(pre_tokenizers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvW7VjTEFeD5",
        "outputId": "62797ad7-8788-4786-ce88-f628eadd847d"
      },
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BertPreTokenizer',\n",
              " 'ByteLevel',\n",
              " 'CharDelimiterSplit',\n",
              " 'Digits',\n",
              " 'Metaspace',\n",
              " 'PreTokenizer',\n",
              " 'Punctuation',\n",
              " 'Sequence',\n",
              " 'Split',\n",
              " 'UnicodeScripts',\n",
              " 'Whitespace',\n",
              " 'WhitespaceSplit',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " 'pre_tokenizers']"
            ]
          },
          "metadata": {},
          "execution_count": 320
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp_tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "sp_tokenizer.normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
        "sp_tokenizer.pre_tokenizer = PreTokenizer.custom(CustomPreTokenizer())\n",
        "sp_tokenizer.decoder = decoders.BPEDecoder(suffix=\"</w>\")\n",
        "class CustomDecoder:\n",
        "    def decode(self, tokens: List[str]) -> str:\n",
        "        return \"\".join(tokens)\n",
        "# sp_tokenizer.post_processor = TemplateProcessing(single=\"[CLS] $A [SEP]\",\n",
        "#                                                  pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "#                                                  special_tokens=[\n",
        "#                                                     (\"[CLS]\", 1),\n",
        "#                                                     (\"[SEP]\", 2),\n",
        "#                                                     (\"[NEW-LINE]\",3)])\n",
        "\n",
        "# trainer = BpeTrainer(vocab_size=500, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "# sp_tokenizer.train_from_iterator(text , trainer)"
      ],
      "metadata": {
        "id": "zmR8b7NddHWb"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(PreTokenizer.custom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA_KOZEr9IUI",
        "outputId": "5e2b79f5-7468-4c37-f439-cbc63903b300"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__name__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__qualname__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__self__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__text_signature__']"
            ]
          },
          "metadata": {},
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sp_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "8AU27va3luOE",
        "outputId": "97c285a9-8e27-4d2d-bf06-b51600c1c6ce"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-268-dc6c12b9d211>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenize_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: Exception: TypeError: NormalizedString.split() missing 1 required positional argument: 'behavior'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toe = CharBPETokenizer()"
      ],
      "metadata": {
        "id": "-8_ulOnGRuG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "7K_9bgAaEuME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_tokenizer.save(\"sp-tokenizer.json\")"
      ],
      "metadata": {
        "id": "Nm3yuCk7EwB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "tokenizer.train_from_iterator(text, trainer)\n",
        "tokenizer.save(\"sp-tokenizer.json\")"
      ],
      "metadata": {
        "id": "1f1qpOz87-iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(text[:50])\n",
        "output.tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbSfnflk8FDm",
        "outputId": "0df0b5c6-d06b-4916-ca06-763539dff04a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'first',\n",
              " 'citizen',\n",
              " ':',\n",
              " 'before',\n",
              " 'we',\n",
              " 'proceed',\n",
              " 'any',\n",
              " 'further',\n",
              " ',',\n",
              " 'hear',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://github.com/huggingface/tokenizers/tree/b24a2fc1781d5da4e6ebcd3ecb5b91edffc0a05f/bindings/python/py_src/tokenizers/implementations\n",
        "\n",
        "https://huggingface.co/docs/tokenizers/api/trainers\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt\n",
        "\n",
        "\n",
        "https://github.com/huggingface/tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "xkPxUc6wdi4Y",
        "outputId": "2be71550-cd56-4e3e-81f2-c5bbe6660033"
      },
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PreTokenize: 永和服装饰品有限公司\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-322-1c7e454b4bb5>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"永和服装饰品有限公司\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PreTokenize:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenize_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;31m# [('永和', (0, 2)), ('服装', (2, 4)), ('饰品', (4, 6)), ('有限公司', (6, 10))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Exception: TypeError: 'str' object cannot be converted to 'NormalizedString'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = list(jieba.tokenize(\"永和服装饰品有限公司\"))\n",
        "type(a[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUfd-mITz4sl",
        "outputId": "ce06da97-f3d7-4240-c5db-1f1ee0723aba"
      },
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "metadata": {},
          "execution_count": 297
        }
      ]
    }
  ]
}