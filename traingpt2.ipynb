{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFn9fvnv/v4JxqqCFFsELk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZXGlz9IoIEZ",
        "outputId": "b6ac8e30-464c-4f3b-bdd6-dc6927e1214b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.13.3\n"
          ]
        }
      ],
      "source": [
        "# !pip install tokenizers\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tokenizers import Tokenizer,PreTokenizedString,NormalizedString, pre_tokenizers, CharBPETokenizer , decoders,  normalizers\n",
        "from tokenizers.models import BPE , WordPiece\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace, PreTokenizer\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
        "torch.manual_seed(45562)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8lih-fmoQik",
        "outputId": "9187bb0b-01d2-4588-c089-053f4ba1f79e"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6fa5ac3590>"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the dataset"
      ],
      "metadata": {
        "id": "VUqBry_gJGRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('input.txt', 'r', encoding='utf-8').read()\n",
        "print(\"length of dataset in characters: \", len(text))\n",
        "# print(f\"\\n{text[:200]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSRy14hJJFoi",
        "outputId": "dd033bb3-7e77-4fcd-d0a4-fac99411e379"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a custom tokenizer with hugging face  tokenizer API"
      ],
      "metadata": {
        "id": "DnGgF5W0I4x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.Split(\"\\n\",'merged_with_previous'),Whitespace()])"
      ],
      "metadata": {
        "id": "T-oBF0Y_aJWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_tokenizer.pre_tokenize_str(text[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujmb0dxhasjR",
        "outputId": "b5573b03-132d-434f-aefe-d3c976b49bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('First Citizen:\\n', (0, 15)),\n",
              " ('Before we proceed any further, hear', (15, 50))]"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomPreTokenizer:\n",
        "    \"A pre_tokenizer that preserves the preserves the new line character\"\n",
        "    def line_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
        "        splits = []\n",
        "        for token in normalized_string.split(\" \"):\n",
        "          if \"\\n\" in token:\n",
        "              subtokens = token.split(\"\\n\")\n",
        "              for i, subtoken in enumerate(subtokens):\n",
        "                  if i > 0:\n",
        "                      splits.append(\"\\n\")\n",
        "                  if subtoken:\n",
        "                      splits.append(subtoken)\n",
        "          else:\n",
        "              splits.append(token)\n",
        "        return [NormalizedString(split).split(behavior=\"N\") for split in splits]\n",
        "\n",
        "    # def pre_tokenize(self, pretok):\n",
        "    #     pretok.split(self.line_split)#.line_split(pretok)\n",
        "\n",
        "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
        "        pretok.split(self.line_split)"
      ],
      "metadata": {
        "id": "mhJ7SvNScEAS"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VeyTUms67F8",
        "outputId": "69a67da3-4681-47e4-99fe-15e5a6da29b0"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['First',\n",
              " 'Citizen:',\n",
              " '\\n',\n",
              " 'Before',\n",
              " 'we',\n",
              " 'proceed',\n",
              " 'any',\n",
              " 'further,',\n",
              " 'hear']"
            ]
          },
          "metadata": {},
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok = Tokenizer(BPE())\n",
        "tok.normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
        "tok.pre_tokenizer = PreTokenizer.custom(CustomPreTokenizer())\n",
        "# tok.decoder = Decoder.custom(CustomDecoder())\n",
        "print(tok.pre_tokenizer.pre_tokenize_str(text[:50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "gjbsuF4a366z",
        "outputId": "2a0998bf-7448-4fb3-f7bf-e620ee4b17c5"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-289-0ede6c9d5036>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustomPreTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# tok.decoder = Decoder.custom(CustomDecoder())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenize_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: Exception: TypeError: NormalizedString.split() missing 1 required positional argument: 'behavior'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp_tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "sp_tokenizer.normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
        "sp_tokenizer.pre_tokenizer = PreTokenizer.custom(CustomPreTokenizer())\n",
        "sp_tokenizer.decoder = decoders.BPEDecoder(suffix=\"</w>\")\n",
        "class CustomDecoder:\n",
        "    def decode(self, tokens: List[str]) -> str:\n",
        "        return \"\".join(tokens)\n",
        "# sp_tokenizer.post_processor = TemplateProcessing(single=\"[CLS] $A [SEP]\",\n",
        "#                                                  pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "#                                                  special_tokens=[\n",
        "#                                                     (\"[CLS]\", 1),\n",
        "#                                                     (\"[SEP]\", 2),\n",
        "#                                                     (\"[NEW-LINE]\",3)])\n",
        "\n",
        "# trainer = BpeTrainer(vocab_size=500, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "# sp_tokenizer.train_from_iterator(text , trainer)"
      ],
      "metadata": {
        "id": "zmR8b7NddHWb"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(PreTokenizer.custom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA_KOZEr9IUI",
        "outputId": "5e2b79f5-7468-4c37-f439-cbc63903b300"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__name__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__qualname__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__self__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__text_signature__']"
            ]
          },
          "metadata": {},
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sp_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "8AU27va3luOE",
        "outputId": "97c285a9-8e27-4d2d-bf06-b51600c1c6ce"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-268-dc6c12b9d211>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenize_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: Exception: TypeError: NormalizedString.split() missing 1 required positional argument: 'behavior'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toe = CharBPETokenizer()"
      ],
      "metadata": {
        "id": "-8_ulOnGRuG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "7K_9bgAaEuME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_tokenizer.save(\"sp-tokenizer.json\")"
      ],
      "metadata": {
        "id": "Nm3yuCk7EwB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "tokenizer.train_from_iterator(text, trainer)\n",
        "tokenizer.save(\"sp-tokenizer.json\")"
      ],
      "metadata": {
        "id": "1f1qpOz87-iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(text[:50])\n",
        "output.tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbSfnflk8FDm",
        "outputId": "0df0b5c6-d06b-4916-ca06-763539dff04a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'first',\n",
              " 'citizen',\n",
              " ':',\n",
              " 'before',\n",
              " 'we',\n",
              " 'proceed',\n",
              " 'any',\n",
              " 'further',\n",
              " ',',\n",
              " 'hear',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "\n",
        "from typing import List\n",
        "\n",
        "from tokenizers import Tokenizer, Regex, NormalizedString, PreTokenizedString\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import PreTokenizer\n",
        "from tokenizers.normalizers import Normalizer\n",
        "from tokenizers.decoders import Decoder\n",
        "\n",
        "\n",
        "class JiebaPreTokenizer:\n",
        "    def jieba_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
        "        splits = []\n",
        "        # we need to call `str(normalized_string)` because jieba expects a str,\n",
        "        # not a NormalizedString\n",
        "        for token in normalized_string:\n",
        "            splits.append(token)\n",
        "\n",
        "        return splits\n",
        "        # We can also easily do it in one line:\n",
        "        # return [normalized_string[w[1] : w[2]] for w in jieba.tokenize(str(normalized_string))]\n",
        "\n",
        "    # def odd_number_split( self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
        "    #     # Just an odd example...\n",
        "    #     splits = []\n",
        "    #     last = 0\n",
        "    #     for (i, char) in enumerate(str(normalized_string)):\n",
        "    #         if char.isnumeric() and int(char) % 2 == 1:\n",
        "    #             splits.append(normalized_string[last:i])\n",
        "    #             last = i\n",
        "    #     # Don't forget the last one\n",
        "    #     splits.append(normalized_string[last:])\n",
        "    #     return splits\n",
        "\n",
        "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
        "        # Let's call split on the PreTokenizedString to split using `self.jieba_split`\n",
        "        pretok.split(self.jieba_split)\n",
        "        # Here we can call `pretok.split` multiple times if we want to apply\n",
        "        # different algorithm, but we generally just need to call it once.\n",
        "        # pretok.split(self.odd_number_split)\n",
        "\n",
        "\n",
        "class CustomDecoder:\n",
        "    def decode(self, tokens: List[str]) -> str:\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "\n",
        "class CustomNormalizer:\n",
        "    def normalize(self, normalized: NormalizedString):\n",
        "        # Most of these can be replaced by a `Sequence` combining some provided Normalizer,\n",
        "        # (ie Sequence([ NFKC(), Replace(Regex(\"\\s+\"), \" \"), Lowercase() ])\n",
        "        # and it should be the prefered way. That being said, here is an example of the kind\n",
        "        # of things that can be done here:\n",
        "        normalized.nfkc()\n",
        "        normalized.filter(lambda char: not char.isnumeric())\n",
        "        normalized.replace(Regex(\"\\s+\"), \" \")\n",
        "        normalized.lowercase()\n",
        "\n",
        "\n",
        "# This section shows how to attach these custom components to the Tokenizer\n",
        "tok = Tokenizer(BPE())\n",
        "tok.normalizer = Normalizer.custom(CustomNormalizer())\n",
        "tok.pre_tokenizer = PreTokenizer.custom(JiebaPreTokenizer())\n",
        "tok.decoder = Decoder.custom(CustomDecoder())\n",
        "\n",
        "input = \"永和服装饰品有限公司\"\n",
        "print(\"PreTokenize:\", input)\n",
        "print(tok.pre_tokenizer.pre_tokenize_str(input))\n",
        "# [('永和', (0, 2)), ('服装', (2, 4)), ('饰品', (4, 6)), ('有限公司', (6, 10))]\n",
        "\n",
        "input = \"112233\"\n",
        "print(\"PreTokenize:\", input)\n",
        "print(tok.pre_tokenizer.pre_tokenize_str(input))\n",
        "# [('1', (0, 1)), ('122', (1, 4)), ('3', (4, 5)), ('3', (5, 6))]\n",
        "\n",
        "input = \"1234 ℌ𝔢𝔩𝔩𝔬    𝔱𝔥𝔢𝔯𝔢 𝓂𝓎 𝒹ℯ𝒶𝓇 𝕕𝕖𝕒𝕣    𝕗𝕣𝕚𝕖𝕟𝕕!\"\n",
        "print(\"Normalize:\", input)\n",
        "print(tok.normalizer.normalize_str(input))\n",
        "# \" hello there my dear dear friend!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkPxUc6wdi4Y",
        "outputId": "2d1e0980-7b67-4146-d33f-719c00a2dbae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PreTokenize: 永和服装饰品有限公司\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(jieba.tokenize(str(\"永和服装饰品有限公司\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUfd-mITz4sl",
        "outputId": "f7632c0d-95d8-46c7-bb4a-1b5acf600ad0"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('永和', 0, 2), ('服装', 2, 4), ('饰品', 4, 6), ('有限公司', 6, 10)]"
            ]
          },
          "metadata": {},
          "execution_count": 275
        }
      ]
    }
  ]
}