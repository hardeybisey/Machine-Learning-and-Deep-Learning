{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCBJneyCNVwKQXyEveU+1W"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZXGlz9IoIEZ"
      },
      "outputs": [],
      "source": [
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the dataset"
      ],
      "metadata": {
        "id": "VUqBry_gJGRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = open('input.txt', 'r', encoding='utf-8').read()\n",
        "print(\"length of dataset in characters: \", len(corpus))\n",
        "print(f\"\\n{corpus[:200]}\")"
      ],
      "metadata": {
        "id": "sSRy14hJJFoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a custom tokenizer with hugging face  tokenizer API"
      ],
      "metadata": {
        "id": "DnGgF5W0I4x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "for word in re.findall(r'\\b\\w+\\b|\\n|[^\\s\\w]', corpus)\n",
        "    word_freqs[word] += 1\n",
        "\n",
        "alphabet = []\n",
        "for word in word_freqs.keys():\n",
        "    if word[0] not in alphabet:\n",
        "        alphabet.append(word[0])\n",
        "    for letter in word[1:]:\n",
        "        if f\"##{letter}\" not in alphabet:\n",
        "            alphabet.append(f\"##{letter}\")\n",
        "\n",
        "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n",
        "\n",
        "splits = {\n",
        "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
        "    for word in word_freqs.keys()}\n",
        "\n",
        "def compute_pair_scores(splits):\n",
        "    letter_freqs = defaultdict(int)\n",
        "    pair_freqs = defaultdict(int)\n",
        "    for word, freq in word_freqs.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            letter_freqs[split[0]] += freq\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            letter_freqs[split[i]] += freq\n",
        "            pair_freqs[pair] += freq\n",
        "        letter_freqs[split[-1]] += freq\n",
        "\n",
        "    scores = { pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]]) for pair, freq in pair_freqs.items() }\n",
        "    return scores\n",
        "\n",
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
        "                split = split[:i] + [merge] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits\n",
        "\n",
        "pair_scores = compute_pair_scores(splits)\n",
        "best_pair = \"\"\n",
        "max_score = None\n",
        "for pair, score in pair_scores.items():\n",
        "    if max_score is None or max_score < score:\n",
        "        best_pair = pair\n",
        "        max_score = score\n",
        "vocab_size = 700\n",
        "while len(vocab) < vocab_size:\n",
        "    scores = compute_pair_scores(splits)\n",
        "    best_pair, max_score = \"\", None\n",
        "    for pair, score in scores.items():\n",
        "        if max_score is None or max_score < score:\n",
        "            best_pair = pair\n",
        "            max_score = score\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    new_token = ( best_pair[0] + best_pair[1][2:] if best_pair[1].startswith(\"##\") else best_pair[0] + best_pair[1] )\n",
        "    vocab.append(new_token)\n",
        "\n",
        "def encode_word(word):\n",
        "    tokens = []\n",
        "    while len(word) > 0:\n",
        "        i = len(word)\n",
        "        while i > 0 and word[:i] not in vocab:\n",
        "            i -= 1\n",
        "        if i == 0:\n",
        "            return [\"[UNK]\"]\n",
        "        tokens.append(word[:i])\n",
        "        word = word[i:]\n",
        "        if len(word) > 0:\n",
        "            word = f\"##{word}\"\n",
        "    return tokens\n",
        "\n",
        "def tokenize(text):\n",
        "    pre_tokenized_text = re.findall(r'\\b\\w+\\b|\\n|[^\\s\\w]', text)\n",
        "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
        "    return sum(encoded_words, [])"
      ],
      "metadata": {
        "id": "zmR8b7NddHWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# worked\n",
        "# tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "# tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
        "# tokenizer.normalizer = normalizers.Sequence(\n",
        "#     [normalizers.NFD(), normalizers.StripAccents()])\n",
        "# tokenizer.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.Split('', 'isolated'),\n",
        "#                                                    pre_tokenizers.Punctuation()])\n",
        "\n",
        "# tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my \\n pre-tokenizer.\")\n",
        "# special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "# trainer = trainers.WordPieceTrainer(vocab_size=5000, special_tokens=special_tokens)\n",
        "# tokenizer.train_from_iterator(text, trainer=trainer)"
      ],
      "metadata": {
        "id": "xkPxUc6wdi4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.encode(text[:50]).ids) \n",
        "print(tokenizer.encode(text[:50]).tokens)"
      ],
      "metadata": {
        "id": "ufPoR23CALFy",
        "outputId": "82ee54af-ad60-401c-e172-fd43efcc2ed2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23, 52, 61, 62, 63, 6, 20, 52, 63, 52, 69, 48, 57, 15, 5, 19, 48, 49, 58, 61, 48, 6, 66, 48, 6, 59, 61, 58, 46, 48, 48, 47, 6, 44, 57, 68, 6, 49, 64, 61, 63, 51, 48, 61, 11, 6, 51, 48, 44, 61]\n",
            "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r']\n"
          ]
        }
      ]
    }
  ]
}