{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6GN7G9bbX7GTY0yMlb77D"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZXGlz9IoIEZ",
        "outputId": "b6ac8e30-464c-4f3b-bdd6-dc6927e1214b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.13.3\n"
          ]
        }
      ],
      "source": [
        "# !pip install tokenizers\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tokenizers import Tokenizer,PreTokenizedString,NormalizedString\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "torch.manual_seed(45562)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8lih-fmoQik",
        "outputId": "eacdb3ce-1360-41a0-bfd2-1e33a729d91b"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6fa5ac3590>"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, pre_tokenizers, CharBPETokenizer , decoders,  normalizers\n",
        "from tokenizers.pre_tokenizers import PreTokenizer\n",
        "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.trainers import BpeTrainer"
      ],
      "metadata": {
        "id": "8wkoNctqFs5D"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the dataset"
      ],
      "metadata": {
        "id": "VUqBry_gJGRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('input.txt', 'r', encoding='utf-8').read()\n",
        "print(\"length of dataset in characters: \", len(text))\n",
        "# print(f\"\\n{text[:200]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSRy14hJJFoi",
        "outputId": "f5037187-fd53-452f-eb4b-bd2fea2b803c"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a custom tokenizer with hugging face  tokenizer API"
      ],
      "metadata": {
        "id": "DnGgF5W0I4x9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wUXitofsJC9A"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.Split(\"\\n\",'merged_with_previous'),Whitespace()])"
      ],
      "metadata": {
        "id": "T-oBF0Y_aJWA"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_tokenizer.pre_tokenize_str(text[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujmb0dxhasjR",
        "outputId": "b5573b03-132d-434f-aefe-d3c976b49bfd"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('First Citizen:\\n', (0, 15)),\n",
              " ('Before we proceed any further, hear', (15, 50))]"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomPreTokenizer:\n",
        "    \"A pre_tokenizer that preserves the preserves the new line character\"\n",
        "    def line_split(self,i: int, normalized_string: NormalizedString):\n",
        "        splits = []\n",
        "        for subtokens in normalized_string.split(\"\\n\"):\n",
        "            for i, subtoken in enumerate(subtokens):\n",
        "                if i > 0:\n",
        "                    tokens.append(\"\\n\")\n",
        "                if subtoken:\n",
        "                    tokens.append(subtoken)\n",
        "            else:\n",
        "                splits.append(token)\n",
        "        return splits\n",
        "      \n",
        "    def whitespace_split(self, i: int, normalized_string: NormalizedString):\n",
        "        splits = [token for token in normalized_string.split(\" \")]\n",
        "        return splits\n",
        "\n",
        "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
        "        pretok.split(self.line_split, )\n",
        "        pretok.split(self.whitespace_split)"
      ],
      "metadata": {
        "id": "mhJ7SvNScEAS"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'aawdfghhf'.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG2onMEJorcP",
        "outputId": "fdfa6032-f15c-429c-bf6f-7d552f025981"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aawdfghhf']"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp_tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "sp_tokenizer.normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
        "sp_tokenizer.pre_tokenizer = PreTokenizer.custom(CustomPreTokenizer())\n",
        "sp_tokenizer.decoder = decoders.BPEDecoder(suffix=\"</w>\")\n",
        "sp_tokenizer.post_processor = TemplateProcessing(single=\"[CLS] $A [SEP]\",\n",
        "                                                 pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "                                                 special_tokens=[\n",
        "                                                    (\"[CLS]\", 1),\n",
        "                                                    (\"[SEP]\", 2),\n",
        "                                                    (\"[NEW-LINE]\",3)])\n",
        "\n",
        "trainer = BpeTrainer(vocab_size=500, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "sp_tokenizer.train_from_iterator(text , trainer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "zmR8b7NddHWb",
        "outputId": "c9c8c5bc-2383-4269-def8-1b936936490f"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-220-76c673806200>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBpeTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"[UNK]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[CLS]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[SEP]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[PAD]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[MASK]\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msp_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: Exception: TypeError: NormalizedString.split() missing 1 required positional argument: 'behavior'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sp_tokenizer.pre_tokenizer.pre_tokenize_str(text[:50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "8AU27va3luOE",
        "outputId": "b92a4d80-2b3e-4ac4-ea20-2bc4d874db24"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-222-a9bc169b0843>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenize_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: Exception: TypeError: NormalizedString.split() missing 1 required positional argument: 'behavior'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.pre_tokenize_str(text[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "fM6GBk3ml6xM",
        "outputId": "600f1c82-d654-40e7-f7e4-16c45ebc7726"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-218-51846e8f613d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenize_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: Exception: TypeError: NormalizedString.split() missing 1 required positional argument: 'behavior'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CustomPreTokenizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "AWD-LGt3kHCm",
        "outputId": "1f93e8f0-ad03-4c2e-b05b-0702f0f86d69"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-205-5ca5a9bb8771>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCustomPreTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: No constructor defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "for token in text[:50].split(' '):\n",
        "      if \"\\n\" in token:\n",
        "          subtokens = token.split(\"\\n\")\n",
        "          print(subtokens)\n",
        "          for i, subtoken in enumerate(subtokens):\n",
        "              if i > 0:\n",
        "                  tokens.append(\"\\n\")\n",
        "              if subtoken:\n",
        "                  tokens.append(subtoken)\n",
        "      else:\n",
        "          tokens.append(token)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07nepJT6bnE9",
        "outputId": "95306743-4b61-4f18-c549-607b471b07bf"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Citizen:', 'Before']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kHmGTjCbPOm",
        "outputId": "67f2a5f2-dc29-4dc4-9f28-e44d6def533c"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['First',\n",
              " 'Citizen:',\n",
              " '\\n',\n",
              " 'Before',\n",
              " 'we',\n",
              " 'proceed',\n",
              " 'any',\n",
              " 'further,',\n",
              " 'hear']"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toe = CharBPETokenizer()"
      ],
      "metadata": {
        "id": "-8_ulOnGRuG8"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "7K_9bgAaEuME"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_tokenizer.save(\"sp-tokenizer.json\")"
      ],
      "metadata": {
        "id": "Nm3yuCk7EwB8"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "tokenizer.train_from_iterator(text, trainer)\n",
        "tokenizer.save(\"sp-tokenizer.json\")"
      ],
      "metadata": {
        "id": "1f1qpOz87-iK"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(text[:50])\n",
        "output.tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbSfnflk8FDm",
        "outputId": "0df0b5c6-d06b-4916-ca06-763539dff04a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'first',\n",
              " 'citizen',\n",
              " ':',\n",
              " 'before',\n",
              " 'we',\n",
              " 'proceed',\n",
              " 'any',\n",
              " 'further',\n",
              " ',',\n",
              " 'hear',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokens = nltk.regexp_tokenize(text, pattern = r'\\w+|\\n')\n",
        "\n",
        "# chars = sorted(list(set(tokens)))\n",
        "# # create a mapping from characters to integers\n",
        "# stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "# itos = {i:ch for i,ch in enumerate(chars)}\n",
        "# encode = lambda text: [stoi[c] for c in nltk.regexp_tokenize(text, pattern = r'\\w+|\\n')] \n",
        "# decode = lambda l: ' '.join([itos[i] for i in l])\n",
        "# print(encode(\"\"\"Advocate', 'Affection', 'Affliction'\"\"\"))\n",
        "# print(decode(encode(\"\"\"Advocate', 'Affection', 'Affliction'\"\"\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVjIAIP6odDS",
        "outputId": "aaedf20b-e7f8-4182-9eaa-eefc991f5e0d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[65, 66, 67]\n",
            "Advocate Affection Affliction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "\n",
        "from typing import List\n",
        "\n",
        "from tokenizers import Tokenizer, Regex, NormalizedString, PreTokenizedString\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import PreTokenizer\n",
        "from tokenizers.normalizers import Normalizer\n",
        "from tokenizers.decoders import Decoder\n",
        "\n",
        "\n",
        "class JiebaPreTokenizer:\n",
        "    def jieba_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
        "        splits = []\n",
        "        # we need to call `str(normalized_string)` because jieba expects a str,\n",
        "        # not a NormalizedString\n",
        "        for token, start, stop in jieba.tokenize(str(normalized_string)):\n",
        "            splits.append(normalized_string[start:stop])\n",
        "\n",
        "        return splits\n",
        "        # We can also easily do it in one line:\n",
        "        # return [normalized_string[w[1] : w[2]] for w in jieba.tokenize(str(normalized_string))]\n",
        "\n",
        "    def odd_number_split( self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
        "        # Just an odd example...\n",
        "        splits = []\n",
        "        last = 0\n",
        "        for (i, char) in enumerate(str(normalized_string)):\n",
        "            if char.isnumeric() and int(char) % 2 == 1:\n",
        "                splits.append(normalized_string[last:i])\n",
        "                last = i\n",
        "        # Don't forget the last one\n",
        "        splits.append(normalized_string[last:])\n",
        "        return splits\n",
        "\n",
        "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
        "        # Let's call split on the PreTokenizedString to split using `self.jieba_split`\n",
        "        pretok.split(self.jieba_split)\n",
        "        # Here we can call `pretok.split` multiple times if we want to apply\n",
        "        # different algorithm, but we generally just need to call it once.\n",
        "        pretok.split(self.odd_number_split)\n",
        "\n",
        "\n",
        "class CustomDecoder:\n",
        "    def decode(self, tokens: List[str]) -> str:\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "\n",
        "class CustomNormalizer:\n",
        "    def normalize(self, normalized: NormalizedString):\n",
        "        # Most of these can be replaced by a `Sequence` combining some provided Normalizer,\n",
        "        # (ie Sequence([ NFKC(), Replace(Regex(\"\\s+\"), \" \"), Lowercase() ])\n",
        "        # and it should be the prefered way. That being said, here is an example of the kind\n",
        "        # of things that can be done here:\n",
        "        normalized.nfkc()\n",
        "        normalized.filter(lambda char: not char.isnumeric())\n",
        "        normalized.replace(Regex(\"\\s+\"), \" \")\n",
        "        normalized.lowercase()\n",
        "\n",
        "\n",
        "# This section shows how to attach these custom components to the Tokenizer\n",
        "tok = Tokenizer(BPE())\n",
        "tok.normalizer = Normalizer.custom(CustomNormalizer())\n",
        "tok.pre_tokenizer = PreTokenizer.custom(JiebaPreTokenizer())\n",
        "tok.decoder = Decoder.custom(CustomDecoder())\n",
        "\n",
        "input = \"æ°¸å’Œæœè£…é¥°å“æœ‰é™å…¬å¸\"\n",
        "print(\"PreTokenize:\", input)\n",
        "print(tok.pre_tokenizer.pre_tokenize_str(input))\n",
        "# [('æ°¸å’Œ', (0, 2)), ('æœè£…', (2, 4)), ('é¥°å“', (4, 6)), ('æœ‰é™å…¬å¸', (6, 10))]\n",
        "\n",
        "input = \"112233\"\n",
        "print(\"PreTokenize:\", input)\n",
        "print(tok.pre_tokenizer.pre_tokenize_str(input))\n",
        "# [('1', (0, 1)), ('122', (1, 4)), ('3', (4, 5)), ('3', (5, 6))]\n",
        "\n",
        "input = \"1234 â„Œğ”¢ğ”©ğ”©ğ”¬    ğ”±ğ”¥ğ”¢ğ”¯ğ”¢ ğ“‚ğ“ ğ’¹â„¯ğ’¶ğ“‡ ğ••ğ•–ğ•’ğ•£    ğ•—ğ•£ğ•šğ•–ğ•Ÿğ••!\"\n",
        "print(\"Normalize:\", input)\n",
        "print(tok.normalizer.normalize_str(input))\n",
        "# \" hello there my dear dear friend!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkPxUc6wdi4Y",
        "outputId": "eac03e3d-1823-4df3-e714-d93905c5b6e6"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PreTokenize: æ°¸å’Œæœè£…é¥°å“æœ‰é™å…¬å¸\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.947 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.947 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('æ°¸å’Œ', (0, 2)), ('æœè£…', (2, 4)), ('é¥°å“', (4, 6)), ('æœ‰é™å…¬å¸', (6, 10))]\n",
            "PreTokenize: 112233\n",
            "[('1', (0, 1)), ('122', (1, 4)), ('3', (4, 5)), ('3', (5, 6))]\n",
            "Normalize: 1234 â„Œğ”¢ğ”©ğ”©ğ”¬    ğ”±ğ”¥ğ”¢ğ”¯ğ”¢ ğ“‚ğ“ ğ’¹â„¯ğ’¶ğ“‡ ğ••ğ•–ğ•’ğ•£    ğ•—ğ•£ğ•šğ•–ğ•Ÿğ••!\n",
            " hello there my dear dear friend!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(jieba.tokenize(str(input)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUfd-mITz4sl",
        "outputId": "1554ef7d-04cf-4f46-c8f9-09f10c0709bc"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1234', 0, 4),\n",
              " (' ', 4, 5),\n",
              " ('â„Œ', 5, 6),\n",
              " ('ğ”¢', 6, 7),\n",
              " ('ğ”©', 7, 8),\n",
              " ('ğ”©', 8, 9),\n",
              " ('ğ”¬', 9, 10),\n",
              " (' ', 10, 11),\n",
              " (' ', 11, 12),\n",
              " (' ', 12, 13),\n",
              " (' ', 13, 14),\n",
              " ('ğ”±', 14, 15),\n",
              " ('ğ”¥', 15, 16),\n",
              " ('ğ”¢', 16, 17),\n",
              " ('ğ”¯', 17, 18),\n",
              " ('ğ”¢', 18, 19),\n",
              " (' ', 19, 20),\n",
              " ('ğ“‚', 20, 21),\n",
              " ('ğ“', 21, 22),\n",
              " (' ', 22, 23),\n",
              " ('ğ’¹', 23, 24),\n",
              " ('â„¯', 24, 25),\n",
              " ('ğ’¶', 25, 26),\n",
              " ('ğ“‡', 26, 27),\n",
              " (' ', 27, 28),\n",
              " ('ğ••', 28, 29),\n",
              " ('ğ•–', 29, 30),\n",
              " ('ğ•’', 30, 31),\n",
              " ('ğ•£', 31, 32),\n",
              " (' ', 32, 33),\n",
              " (' ', 33, 34),\n",
              " (' ', 34, 35),\n",
              " (' ', 35, 36),\n",
              " ('ğ•—', 36, 37),\n",
              " ('ğ•£', 37, 38),\n",
              " ('ğ•š', 38, 39),\n",
              " ('ğ•–', 39, 40),\n",
              " ('ğ•Ÿ', 40, 41),\n",
              " ('ğ••', 41, 42),\n",
              " ('!', 42, 43)]"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    }
  ]
}