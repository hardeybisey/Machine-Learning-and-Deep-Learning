{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7aDVjyXwduK9Mc3gxS3eR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hardeybisey/neural-network/blob/main/traingpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZXGlz9IoIEZ"
      },
      "outputs": [],
      "source": [
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "# !pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import CharBPETokenizer"
      ],
      "metadata": {
        "id": "Q-qF8teoZ-hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the dataset"
      ],
      "metadata": {
        "id": "VUqBry_gJGRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = open('input.txt', 'r', encoding='utf-8').read()\n",
        "print(\"length of dataset in characters: \", len(corpus))\n",
        "print(f\"{corpus[:200]}\")"
      ],
      "metadata": {
        "id": "sSRy14hJJFoi",
        "outputId": "447e93ea-6228-40c9-d0d4-4c492080a70c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a custom tokenizer with hugging face  tokenizer API"
      ],
      "metadata": {
        "id": "DnGgF5W0I4x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = CharBPETokenizer()\n",
        "tokenizer.train_from_iterator(corpus)"
      ],
      "metadata": {
        "id": "zmR8b7NddHWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the tokenizer\n",
        "encoded = tokenizer.encode(corpus[:50])\n",
        "print(encoded.tokens)\n",
        "print(tokenizer.decode(encoded.ids))"
      ],
      "metadata": {
        "id": "qT1kPwhEaUp2",
        "outputId": "99717630-6b97-4763-b260-fcdace151be8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['F', 'i', 'r', 's', 't</w>', 'C', 'i', 't', 'i', 'z', 'e', 'n</w>', ':</w>', 'B', 'e', 'f', 'o', 'r', 'e</w>', 'w', 'e</w>', 'p', 'r', 'o', 'c', 'e', 'e', 'd</w>', 'a', 'n', 'y</w>', 'f', 'u', 'r', 't', 'h', 'e', 'r</w>', ',</w>', 'h', 'e', 'a', 'r</w>']\n",
            "First Citizen : Before we proceed any further , hear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# worked\n",
        "# tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "# tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
        "# tokenizer.normalizer = normalizers.Sequence(\n",
        "#     [normalizers.NFD(), normalizers.StripAccents()])\n",
        "# tokenizer.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.Split('', 'isolated'),\n",
        "#                                                    pre_tokenizers.Punctuation()])\n",
        "\n",
        "# tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my \\n pre-tokenizer.\")\n",
        "# special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "# trainer = trainers.WordPieceTrainer(vocab_size=5000, special_tokens=special_tokens)\n",
        "# tokenizer.train_from_iterator(text, trainer=trainer)"
      ],
      "metadata": {
        "id": "xkPxUc6wdi4Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}