{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPX/Q67FoKhqoR/8sjI7e8d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hardeybisey/neural-network/blob/main/Copy_of_traingpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZXGlz9IoIEZ",
        "outputId": "5c2e7dae-20ff-4b2e-9c75-d46629ded4d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.13.3\n",
            "--2023-05-09 21:30:48--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-05-09 21:30:48 (109 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# !pip install tokenizers\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tokenizers import Tokenizer,PreTokenizedString,NormalizedString, pre_tokenizers, CharBPETokenizer , decoders,  normalizers,ByteLevelBPETokenizer\n",
        "from tokenizers.models import BPE , WordPiece\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace, PreTokenizer\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
        "torch.manual_seed(45562)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8lih-fmoQik",
        "outputId": "80d08f96-5212-4205-ff5a-81a359d00898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa4b03c9fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the dataset"
      ],
      "metadata": {
        "id": "VUqBry_gJGRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('input.txt', 'r', encoding='utf-8').read().replace(\"\\n\", \"[NL]\")\n",
        "print(\"length of dataset in characters: \", len(text))\n",
        "# print(f\"\\n{text[:200]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSRy14hJJFoi",
        "outputId": "0407caa4-7854-4c63-b2ed-fe54bdbc45f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1235394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a custom tokenizer with hugging face  tokenizer API"
      ],
      "metadata": {
        "id": "DnGgF5W0I4x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok = ByteLevelBPETokenizer()\n",
        "# tok.normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
        "# tok.pre_tokenizer = PreTokenizer.custom(CustomPreTokenizer())\n",
        "# tok.decoder = Decoder.custom(CustomDecoder())\n",
        "print(tok.pre_tokenizer.pre_tokenize_str(text[:50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjbsuF4a366z",
        "outputId": "dc3ce1a9-efb9-4d55-e31d-5b1c8beb3aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('First', (0, 5)), ('ĠCitizen', (5, 13)), (':<', (13, 15)), ('NL', (15, 17)), ('>', (17, 18)), ('Before', (18, 24)), ('Ġwe', (24, 27)), ('Ġproceed', (27, 35)), ('Ġany', (35, 39)), ('Ġfurther', (39, 47)), (',', (47, 48)), ('Ġh', (48, 50))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(pre_tokenizers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvW7VjTEFeD5",
        "outputId": "62797ad7-8788-4786-ce88-f628eadd847d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BertPreTokenizer',\n",
              " 'ByteLevel',\n",
              " 'CharDelimiterSplit',\n",
              " 'Digits',\n",
              " 'Metaspace',\n",
              " 'PreTokenizer',\n",
              " 'Punctuation',\n",
              " 'Sequence',\n",
              " 'Split',\n",
              " 'UnicodeScripts',\n",
              " 'Whitespace',\n",
              " 'WhitespaceSplit',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " 'pre_tokenizers']"
            ]
          },
          "metadata": {},
          "execution_count": 320
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp_tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "sp_tokenizer.normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
        "sp_tokenizer.pre_tokenizer = PreTokenizer.custom(CustomPreTokenizer())\n",
        "sp_tokenizer.decoder = decoders.BPEDecoder(suffix=\"</w>\")\n",
        "sp_tokenizer.post_processor = TemplateProcessing(single=\"[CLS] $A [SEP]\",\n",
        "                                                 pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "                                                 special_tokens=[\n",
        "                                                    (\"[CLS]\", 1),\n",
        "                                                    (\"[SEP]\", 2),\n",
        "                                                    (\"[NEW-LINE]\",3)])\n",
        "\n",
        "trainer = BpeTrainer(vocab_size=500, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "sp_tokenizer.train_from_iterator(text , trainer)"
      ],
      "metadata": {
        "id": "zmR8b7NddHWb",
        "outputId": "54320d9c-1ebe-4ed4-b22c-856786606bd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-76c673806200>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msp_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munk_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msp_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNFD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStripAccents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msp_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustomPreTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msp_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBPEDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"</w>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m sp_tokenizer.post_processor = TemplateProcessing(single=\"[CLS] $A [SEP]\",\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CustomPreTokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toe = CharBPETokenizer()"
      ],
      "metadata": {
        "id": "-8_ulOnGRuG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "7K_9bgAaEuME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"sp-tokenizer.json\")"
      ],
      "metadata": {
        "id": "Nm3yuCk7EwB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\",\"[NL]\"])\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "tokenizer.decoder = decoders.BPEDecoder(suffix=\"</w>\")\n",
        "tokenizer.train_from_iterator(text, trainer)\n",
        "tokenizer.save(\"sp-tokenizer.json\")"
      ],
      "metadata": {
        "id": "1f1qpOz87-iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(text[:50])\n",
        "output.tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbSfnflk8FDm",
        "outputId": "0df0b5c6-d06b-4916-ca06-763539dff04a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'first',\n",
              " 'citizen',\n",
              " ':',\n",
              " 'before',\n",
              " 'we',\n",
              " 'proceed',\n",
              " 'any',\n",
              " 'further',\n",
              " ',',\n",
              " 'hear',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://github.com/huggingface/tokenizers/tree/b24a2fc1781d5da4e6ebcd3ecb5b91edffc0a05f/bindings/python/py_src/tokenizers/implementations\n",
        "\n",
        "https://huggingface.co/docs/tokenizers/api/trainers\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt\n",
        "\n",
        "\n",
        "https://github.com/huggingface/tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "xkPxUc6wdi4Y",
        "outputId": "2be71550-cd56-4e3e-81f2-c5bbe6660033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PreTokenize: 永和服装饰品有限公司\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-322-1c7e454b4bb5>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"永和服装饰品有限公司\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PreTokenize:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenize_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;31m# [('永和', (0, 2)), ('服装', (2, 4)), ('饰品', (4, 6)), ('有限公司', (6, 10))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Exception: TypeError: 'str' object cannot be converted to 'NormalizedString'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = list(jieba.tokenize(\"永和服装饰品有限公司\"))\n",
        "type(a[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUfd-mITz4sl",
        "outputId": "ce06da97-f3d7-4240-c5db-1f1ee0723aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "metadata": {},
          "execution_count": 297
        }
      ]
    }
  ]
}